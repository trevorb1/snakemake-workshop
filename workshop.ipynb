{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10dba33d-29e8-4161-85a4-61c26f35160f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Snakemake Workshop \n",
    "\n",
    "Trevor Barnes <br>\n",
    "ΔE+ Research Lab, SFU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5ae94",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "This presentation is intended to give an overview of the workflow management tool, [Snakemake](https://snakemake.readthedocs.io/en/stable/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681744e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Audience \n",
    "Examples in this presentation are targeted at energy system modellers who are comfortable working with Python. This workshop will use the [OSeMOSYS](http://www.osemosys.org/) Energy Modelling tool in the examples. With that said, there is no background knowledge needed in energy system modelling to follow this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a7a6f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation \n",
    "\"The Snakemake workflow management system is a tool to create reproducible and scalable data analyses.\" ([ref](https://snakemake.readthedocs.io/en/stable/)). Especially in the field of open-source energy modelling, ensuring data is reproducibale is often a requirment for publication. Introducting a tool to manage your data pipelines ensures your work is consistently reproduciable and allows others to eaisly reporduce your results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942df7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Similar tools\n",
    "Snakemake is a Python implementation of a workflow management tool started from the field of bioinfomatics. Many other competitors exist, such as [nextflow](https://www.nextflow.io/), [airflow](https://github.com/apache/airflow), [galaxy](https://usegalaxy.org/), or [mage](https://www.mage.ai/), often with specific use cases or audiances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ffc636",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## References\n",
    "Throughout this workshop, I will reference information from the following sources:\n",
    "- [Snakemake homepage](https://snakemake.github.io/)\n",
    "- [Snakemake paper](https://f1000research.com/articles/10-33)\n",
    "- [Snakemake docs](https://snakemake.readthedocs.io/en/stable/index.html)\n",
    "- [Reproduciable Data Analytic Workflows](https://lachlandeer.github.io/snakemake-econ-r-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21389e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43479af",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Common Terms \n",
    "- **Workflow**: A full data processing pipeline (a series of linked actions)\n",
    "- **Rule**: Rules decompose the workflow into small steps (for examples, running a single script) \n",
    "- **Directed Acyclic Graph (DAC)**: Defines how rules link together\n",
    "- **Wildcard**: Method to generalize a rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d766ee11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rule Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57280d94",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basics\n",
    "Rules must follow a strict formatting guide. At minimum, a rule must have an \"action\" associated with it. In this context, an \"action\" will be a command invoked by a line of code. \n",
    "\n",
    "```python\n",
    "rule hello_world:\n",
    "    shell:\n",
    "        'echo \"Hello World!\"'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fd978",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inputs and Outputs \n",
    "\n",
    "More often, a rule will also have an (or multiple!) input and output files. \n",
    "\n",
    "```python \n",
    "rule name: \n",
    "    input: \n",
    "        \"input/file/path.csv\"\n",
    "    output:\n",
    "        \"output/file/path.csv\"\n",
    "    shell:\n",
    "        \"python script.py {input} {output}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1dfde",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Actions\n",
    "\n",
    "Actions can be more than just shell commands! They can also directly run python scripts, or directly run python code. For example, all three of these scripts are interchangable (assuming hello_world.py only prints \"hello world\")\n",
    "\n",
    "```python\n",
    "rule hello_world:\n",
    "    shell:\n",
    "        \"python hello_world.py\"\n",
    "```\n",
    "<br>\n",
    "```python \n",
    "rule name: \n",
    "    run:\n",
    "        \"print('hello world')\"\n",
    "```\n",
    "<br>\n",
    "```python \n",
    "rule name: \n",
    "    script:\n",
    "        \"scripts/hello_world.py\"\n",
    "```\n",
    "\n",
    "Note, that is using the `script:` command, the directory structure must be set up correctly. Morevoer, all snakemake variables will be passed into the python script (such as the input and output file(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9eedf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other Commands \n",
    "\n",
    "Rules can have many attributes associated with them (see all [here](https://snakemake.readthedocs.io/en/stable/snakefiles/writing_snakefiles.html#grammar)). For example, the following rule is valid.  \n",
    "\n",
    "```python \n",
    "rule create_capex:\n",
    "    message:\n",
    "        \"Creating Capital Costs Data\"\n",
    "    input:\n",
    "        demand=\"SpecifiedAnnualDemand.csv\"\n",
    "        emissions=\"AnnualEmissionLimit.csv\"\n",
    "    output:\n",
    "        capex=\"CapitalCosts.csv\"\n",
    "    params:\n",
    "        scale_factor=2,\n",
    "    threads: 4\n",
    "    log:\n",
    "        \"logs/create_capex.log\"\n",
    "    conda:\n",
    "        \"envs/osemosys.yaml\"\n",
    "    script:\n",
    "        \"scripts/capex.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149525a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Target Rule\n",
    "\n",
    "The **Target Rule** is the rule that Snakemake tries to execute to. This rule is usually called `all` and should not have any `input` files. More information [here](https://snakemake.readthedocs.io/en/stable/tutorial/basics.html#step-7-adding-a-target-rule).\n",
    "\n",
    "```python \n",
    "rule all:\n",
    "    output:\n",
    "        \"reults/plot.png\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52eb90",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running a Workflow\n",
    "\n",
    "Snakemake workflows are run from the command line. They can be run on the cloud or using HPC infrastructure, however, we will only cover local running in this workshop. Full command line options can be found [here](https://snakemake.readthedocs.io/en/stable/executing/cli.html#command-line-interface).\n",
    "\n",
    "To run a workflow simply type the following into the command line (assuming you have snakemake installed in your conda environment) \n",
    "\n",
    "```bash\n",
    "snakemake --cores 4\n",
    "```\n",
    "\n",
    "This will run the workflow with (up to) 4 cores. You must always specifiy the number of cores to run the workflow with. A shorthand to use all available cores is the command \n",
    "\n",
    "```bash \n",
    "snakemake -c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbf1f3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Directory Structure \n",
    "\n",
    "See [here](https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html) for full information, but in general you need to structure your project as shown.  \n",
    "\n",
    "```bash\n",
    "├── .gitignore\n",
    "├── README.md\n",
    "├── LICENSE.md\n",
    "├── workflow\n",
    "│   ├── rules\n",
    "|   │   ├── module1.smk\n",
    "|   │   └── module2.smk\n",
    "│   ├── envs\n",
    "|   │   ├── tool1.yaml\n",
    "|   │   └── tool2.yaml\n",
    "│   ├── scripts\n",
    "|   │   ├── script1.py\n",
    "|   │   └── script2.R\n",
    "│   ├── notebooks\n",
    "|   │   ├── notebook1.py.ipynb\n",
    "|   │   └── notebook2.r.ipynb\n",
    "│   ├── report\n",
    "|   │   ├── plot1.rst\n",
    "|   │   └── plot2.rst\n",
    "|   └── Snakefile\n",
    "├── config\n",
    "│   ├── config.yaml\n",
    "│   └── some-sheet.tsv\n",
    "├── results\n",
    "└── resources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cf9fa-029b-4c9f-8d4d-42cc8553be0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workflow Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a803a60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model \n",
    "\n",
    "Suppose we have a three technology OSeMOSYS Model. It consists of a hydro powerplant, a solar powerplant, and a natural gas powerplant that all contribute to a single electricity demand. The model is shown below. \n",
    "\n",
    "![Model](workshop/images/model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e103cfb4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Workflow\n",
    "\n",
    "The workflow for your model consists of four steps: \n",
    "1. Run a series of Python scripts to generate scenario data for annual demand, capital costs, emission penalties, and variable fuel costs\n",
    "2. Convert all CSV data into a GNU MathProg datafile using [otoole](https://otoole.readthedocs.io/en/latest/)\n",
    "3. Solve the model through GLPK\n",
    "4. Visualize the total annual installed capacity\n",
    "\n",
    "Within this workflow, the generation of all other parameter data is held constant. Each scenario you intend to run will depend on user defined parameters that affect your parameters described in step one (ie. increased electrifiction scenario, cheap solar scenario, ect...). A graphical overview of the workflow is given below. Note, the varaible cost script must be run after the annual demand script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45beafd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"workshop/images/workflow.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79c539",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classical Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad76af3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Steps to Manually Run the Workflow \n",
    "\n",
    "Note that all scripts mentioned can be found in the `workflow/scripts/` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f132fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Create a Scenario Folder \n",
    "```bash \n",
    "mkdir scenarios \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c008e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Copy of the reference data \n",
    "```bash \n",
    "cp resources/data scenarios/data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97acac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Run the capital costs script\n",
    "In this case we will implement a 1.5 scaling factor on solar panels\n",
    "\n",
    "```bash \n",
    "python workflow/scripts/capital_costs.py scenarios/data/CapitalCosts.csv scenarios/data/CapitalCosts.csv \"SPV\" 1.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c658e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. Run the emission penalty script\n",
    "In this case we will create a linear increase from 0 to 100 $/T\n",
    "\n",
    "```bash \n",
    "python workflow/scripts/emission_penalty.py scenarios/data/EmissionsPenalty.csv scenarios/data/EmissionsPenalty.csv 0 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9606e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5. Run the demand script\n",
    "In this case we will scale the demand by 2\n",
    "\n",
    "```bash \n",
    "python workflow/scripts/demand.py scenarios/data/SpecifiedAnnualDemand.csv scenarios/data/SpecifiedAnnualDemand.csv 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276e94b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6. Run the variable costs script\n",
    "Note that this script **must** be run after the demand script \n",
    "\n",
    "```bash \n",
    "python workflow/scripts/variable_costs.py scenarios/data/SpecifiedAnnualDemand.csv scenarios/data/VariableCosts.csv scenarios/data/VariableCosts.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf017a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 7. Use otoole to create a datafile \n",
    "otoole will simply convert csv data into a GNU MathPog file.  \n",
    "\n",
    "```bash \n",
    "otoole convert csv datafile scenarios/data scenarios/data.txt resources/config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde04ea6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 8. Create a folder to hold result data\n",
    "\n",
    "```bash \n",
    "mkdir scenarios/results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c7b9f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 9. Run GLPK to build the model \n",
    "OSeMOSYS requires the current working directory to have a `results`\n",
    "folder when solving through GLPK, so we first change directories to \n",
    "the scenario folder \n",
    "\n",
    "```bash \n",
    "cd scenarios\n",
    "glpsol -m ../resources/osemosys.txt -d scenarios/\n",
    "cd ..\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e523a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 10. Run capacity plotting script \n",
    "\n",
    "```bash \n",
    "python plot_capacity.py scenarios/results/AnnualCapacity.csv scenarios/AnnualCapacity.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812af65a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 11. View Results \n",
    "\n",
    "The `AnnualCapacity.png` file can be found in the folder `results/scenario`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0024e22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Results](workshop/images/example-result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97e867-cfc6-47f5-89d5-65a54471b41c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Issues with Classical Workflow \n",
    "In many cases we will run each of these scenarios one-by-one, manually chnaging data in the script.\n",
    "- Running many scenarios becomes a nightmare \n",
    "- Easy to make data mistakes \n",
    "- Hard to replicate results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0ac59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Snakemake Workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba3953b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Creating the workflow \n",
    "\n",
    "The solution to this workflow can be found in `workshop/solutions/snakemake-1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7d0e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Create a snakefile\n",
    "\n",
    "Lets automate this process through the use of a snakemake workflow! Snakemake will look for a file (called a `snakefile`) located either in the root directory or the `workflow` directory. This file has already been created at `workflow/snakefile`. The `snakefile` will hold all the logic in the workflow. \n",
    "\n",
    "Note, while the file does not have to be called `snakefile`, Snakemake will automatically look for a file called `snakefile` unless specified otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16cac4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Import libraries\n",
    "\n",
    "In the `snakefile`, we will import libraries as normally done in python \n",
    "\n",
    "```python\n",
    "import os\n",
    "import shutil\n",
    "from snakemake.utils import min_version\n",
    "min_version(\"6.0\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d7e0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Create a \"target\" rule\n",
    "In the `snakefile`, create a target rule. This rule holds the final outcome of our workflow, in this case the `AnnualCapacity.csv`. \n",
    "\n",
    "```python\n",
    "rule all:\n",
    "    input:\n",
    "        \"results/scenario/AnnualCapacity.png\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dacd8cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. Create the rules to execute the scripts \n",
    "\n",
    "```python\n",
    "rule capital_cost:\n",
    "    input:\n",
    "        \"resources/data/AnnualDemand.csv\"\n",
    "    output:\n",
    "        \"results/scenario/data/AnnualDemand.csv\"\n",
    "    params:\n",
    "        technology = \"SPV\",\n",
    "        scaling_factor = 1.5\n",
    "    shell: \n",
    "        \"python workflow/scripts/capital_costs.py {input} {output} {params.technology} {params.scaling_factor}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d5e48",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "rule emission_penalty:\n",
    "    input:\n",
    "        \"resources/data/EmissionsPenalty.csv\"\n",
    "    output:\n",
    "        \"results/scenario/data/EmissionsPenalty.csv\"\n",
    "    params:\n",
    "        start = 0,\n",
    "        end = 100\n",
    "    shell: \n",
    "        \"python workflow/scripts/emission_penalty.py {input} {output} {params.start} {params.end}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c7a8d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "rule demand:\n",
    "    input:\n",
    "        \"resources/data/AnnualDemand.csv\"\n",
    "    output:\n",
    "        \"results/scenario/data/AnnualDemand.csv\"\n",
    "    params:\n",
    "        scaling_factor = 2,\n",
    "    shell: \n",
    "        \"python workflow/scripts/demand.py {input} {output} {params.scaling_factor}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9411f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "rule variable_cost:\n",
    "    input:\n",
    "        var_cost = \"resources/data/VariableCosts.csv\",\n",
    "        demand = \"results/scenario/data/AnnualDemand.csv\"\n",
    "    output:\n",
    "        \"results/scenario/data/VariableCosts.csv\"\n",
    "    shell: \n",
    "        \"python workflow/scripts/variable_costs.py {input.demand} {input.var_cost} {output}\"\n",
    "```\n",
    "\n",
    "Note! That the `variable_cost` rule must take in the output from the `demand` rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e893bc-cc28-427f-9d39-f9b3c2d7927c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### 5. Copy reference data rule\n",
    "\n",
    "In general, Snakemake rule outputs should be unique, meaning that the same file shouldn't be created through multiple rules (unless other conditions are imposed). Therefore, we need to be careful not to create a rule that also outputs the files `AnnualDemand.csv`, `CapitalCosts.csv`, `EmissionsPenalty.csv`, and `VariableCosts.csv`. Note that outputs do not necessarly have to be unique, but you will need to ensure you manage AmbigiousRuleOrder exceptions (more info [here](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#handling-ambiguous-rules)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5ddab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first create constants of the files we need: \n",
    "\n",
    "```python\n",
    "OSEMOSYS_CSVS = os.listdir(\"resources/data\")\n",
    "CSVS_TO_CREATE = [\n",
    "    \"AnnualDemand.csv\",\n",
    "    \"EmissionsPenalty.csv\",\n",
    "    \"VariableCosts.csv\",\n",
    "    \"CapitalCosts.csv\"\n",
    "]\n",
    "CSVS_TO_COPY = [f for f in OSEMOSYS_CSVS if f not in CSVS_TO_CREATE]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a09f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then we create a rule to copy over the files in `CSVS_TO_COPY` list. Note here that we are directly executing python code in the rule through the `run:` command. \n",
    "\n",
    "```python \n",
    "rule copy_csv_files:\n",
    "    input:\n",
    "        expand(\"resources/data/{csv}\", csv=OSEMOSYS_CSVS)\n",
    "    output:\n",
    "        expand(\"results/scenario/data/{csv}\", csv=CSVS_TO_COPY)\n",
    "    params:\n",
    "        folder = directory(\"results/scenario/data\")\n",
    "    run:\n",
    "        for path in input:\n",
    "            _, f = os.path.split(path) # f will be in form of \"file.csv\"\n",
    "            if f in CSVS_TO_CREATE:\n",
    "                continue\n",
    "            shutil.copy(path, os.path.join(params.folder, f))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c91471",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6. Create the datafile  \n",
    "```python \n",
    "rule otoole:\n",
    "    input:\n",
    "        expand(\"results/scenario/data/{csv}\", csv=OSEMOSYS_CSVS)\n",
    "    output:\n",
    "        \"results/scenario/data.txt\"\n",
    "    params:\n",
    "        csv_dir = \"results/scenario/data\",\n",
    "        config=\"resources/config.yaml\"\n",
    "    shell:\n",
    "        \"otoole convert csv datafile {params.csv_dir} {output} {params.config}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f1947",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 7. Solve the model rule\n",
    "\n",
    "When using GLPK to solve OSeMOSYS models, by default a results directory must exist in the directory of running. The shell code simply changes the working directory to the location of the scenario before invoking `glpsol`.\n",
    "\n",
    "```python \n",
    "rule solve:\n",
    "    input: \n",
    "        \"results/scenario/data.txt\"\n",
    "    output:\n",
    "        \"results/scenario/results/TotalCapacityAnnual.csv\"\n",
    "    params:\n",
    "        model=\"resources/osemosys.txt\"\n",
    "    shell:\n",
    "        \"\"\"\n",
    "        FILE=\"resources/data.txt\" &&\n",
    "        f=\"$(basename -- $FILE)\" &&\n",
    "        cd results/scenario &&\n",
    "        glpsol -m ../../{params.model} -d $f\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cee85",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 8. Plot results rule\n",
    "```python\n",
    "rule plot:\n",
    "    input:\n",
    "        \"results/scenario/results/TotalCapacityAnnual.csv\"\n",
    "    output:\n",
    "        \"results/scenario/AnnualCapacity.png\"\n",
    "    shell:\n",
    "        \"python workflow/scripts/plot_capacity.py {input} {output} 'Scenario'\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab3de3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 9. Create missing directories rule\n",
    "\n",
    "Throughout this workflow, it is good to ensure that directories exist before running commands. We can do this using the `directory` command in snakemake. \n",
    "\n",
    "```python\n",
    "rule create_scenario_dir:\n",
    "    output:\n",
    "        directory(\"results/scenario/data\")\n",
    "    shell:\n",
    "        \"mkdir output\"\n",
    "```\n",
    "<br>\n",
    "```python\n",
    "rule results_dir:\n",
    "    input:\n",
    "        \"results/scenario\"\n",
    "    output:\n",
    "        directory(\"results/scenario/results\")\n",
    "    shell:\n",
    "        \"mkdir {output}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59053ea9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualize the workflow \n",
    "Before executing the workflow, lets visualize it! This will create the DAG and save it as `dag.pdf` in the root directory\n",
    "\n",
    "```bash\n",
    "snakemake --dag all | dot -Tpdf > dag.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879425ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![dag-1](workshop/images/snakemake-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8d26d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Execute the workflow \n",
    "We can now FINALLY execute the workflow! Lets run it using all available cores. \n",
    "\n",
    "```bash \n",
    "snakemake -c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e138caa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At the start of the run, Snakemake will print out all the rules that it will run. \n",
    "\n",
    "```bash\n",
    "Building DAG of jobs...\n",
    "Using shell: /usr/bin/bash\n",
    "Provided cores: 4\n",
    "Rules claiming more threads will be scaled down.\n",
    "Job stats:\n",
    "job                 count    min threads    max threads\n",
    "----------------  -------  -------------  -------------\n",
    "all                     1              1              1\n",
    "capital_cost            1              1              1\n",
    "copy_csv_files          1              1              1\n",
    "demand                  1              1              1\n",
    "emission_penalty        1              1              1\n",
    "otoole                  1              1              1\n",
    "plot                    1              1              1\n",
    "solve                   1              1              1\n",
    "variable_cost           1              1              1\n",
    "total                   9              1              1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fcf5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After running, we can check the directory structre for our results. \n",
    "\n",
    "![folder-0.png](workshop/images/folder-structure-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969202da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Issues \n",
    "We are still hardcoding in variable values. This is a reproduciable workflow, but not a flexible workflow. Lets fix that with wildcards! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6caac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wildcards\n",
    "\n",
    "The solution to this workflow can be found in `workshop/solutions/snakemake-2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b77bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usually, it is useful to generalize a rule to be applicable to a number of e.g. datasets. For this purpose, wildcards can be used. Automatically resolved multiple named wildcards are a key feature and strength of Snakemake in comparison to other systems. (See more [here](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#wildcards)). \n",
    "\n",
    "Wildcards are identified by enclosing them in curly brackets. For example `{scenario}` would be interpreted as a wildcard to `Snakemake`. Note that the target rule can **not** include wildcards. We need to explicitly tell Snakemake what files we want to create in the target rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f96446",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Replace `scenario` as a wildcard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965d684",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Change scenario in all non-target rules\n",
    "\n",
    "In general, the following rule is how add a wildcard\n",
    "\n",
    "```python\n",
    "rule capital_cost:\n",
    "    input:\n",
    "        \"resources/data/CapitalCost.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/data/CapitalCost.csv\"\n",
    "    params:\n",
    "        technology = \"SPV\",\n",
    "        scaling_factor = 1.5\n",
    "    shell: \n",
    "        \"python workflow/scripts/capital_costs.py {input} {output} {params.technology} {params.scaling_factor}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce2b05",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If your rule includes another function (such as `expand()`), enclose the wildcard in additional curly braces.\n",
    "\n",
    "```python\n",
    "rule otoole:\n",
    "    input:\n",
    "        expand(\"results/{{scenario}}/data/{csv}\", csv=OSEMOSYS_CSVS)\n",
    "    output:\n",
    "        \"results/{scenario}/data.txt\"\n",
    "    params:\n",
    "        csv_dir = \"results/{scenario}/data\",\n",
    "        config=\"resources/config.yaml\"\n",
    "    shell:\n",
    "        \"otoole convert csv datafile {params.csv_dir} {output} {params.config}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415bf0cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you wildcard needs to be accessed through a shell command (or input function), explicitly identify it with `{wildcards.scenario}`\n",
    "\n",
    "```python\n",
    "rule plot:\n",
    "    input:\n",
    "        \"results/{scenario}/results/TotalCapacityAnnual.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/AnnualCapacity.png\"\n",
    "    shell:\n",
    "        \"python workflow/scripts/plot_capacity.py {input} {output} {wildcards.scenario}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfe397",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Update the Target Rule\n",
    "\n",
    "Note, that we now need to explicitly tell Snakemake what files it should generate. \n",
    "\n",
    "```python\n",
    "SCENARIOS = [\"Kamaria\", \"Teddy\", \"Pierre\", \"Narges\", \"Yalda\", \"Trevor\", \"Elias\"]\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"results/{scenario}/AnnualCapacity.png\", scenario=SCENARIOS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8cba1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualize the workflow \n",
    "\n",
    "```bash\n",
    "snakemake --dag all | dot -Tpdf > dag.pdf\n",
    "```\n",
    "\n",
    "How do you expect it to change? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222d45d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![dag-2](workshop/images/snakemake-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e0d9e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Run the workflow \n",
    "\n",
    "```bash \n",
    "snakemake -c\n",
    "```\n",
    "\n",
    "We can see that 57 steps are needed to complete the workflow \n",
    "\n",
    "```bash\n",
    "Building DAG of jobs...\n",
    "Using shell: /usr/bin/bash\n",
    "Provided cores: 4\n",
    "Rules claiming more threads will be scaled down.\n",
    "Job stats:\n",
    "job                 count    min threads    max threads\n",
    "----------------  -------  -------------  -------------\n",
    "all                     1              1              1\n",
    "capital_cost            7              1              1\n",
    "copy_csv_files          7              1              1\n",
    "demand                  7              1              1\n",
    "emission_penalty        7              1              1\n",
    "otoole                  7              1              1\n",
    "plot                    7              1              1\n",
    "solve                   7              1              1\n",
    "variable_cost           7              1              1\n",
    "total                  57              1              1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c62799",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets look at how our folder structure changed! \n",
    "\n",
    "![folder-1.png](workshop/images/folder-structure-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cdd3ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![folder-2.png](workshop/images/folder-structure-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f81c68",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Issues \n",
    "\n",
    "We are hardcoding in scenario values into our `snakefile`. This is bad practice. Lets create a configuration file! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7eb7ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Configuration File\n",
    "\n",
    "The solution to this workflow can be found in `workshop/solutions/snakemake-3` and `workshop/solutions/config-1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21058edc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Configuartion Setup \n",
    "First, set up a `config/config.yaml` file\n",
    "\n",
    "```yaml\n",
    "  scenario_one:\n",
    "    capex:\n",
    "      tech: \"SPV\"\n",
    "      scale: 1.5\n",
    "    emission_penalty:\n",
    "      start: 0\n",
    "      end: 100\n",
    "    demand:\n",
    "      scale: 2\n",
    "  scenario_two:\n",
    "    capex:\n",
    "      tech: \"HYD\"\n",
    "      scale: 5\n",
    "    emission_penalty:\n",
    "      start: 0\n",
    "      end: 10\n",
    "    demand:\n",
    "      scale: 3\n",
    "  scenario_three:\n",
    "    ...\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe307a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Import the configuration file  \n",
    "\n",
    "In the `snakefile`, add the following line\n",
    "\n",
    "```python\n",
    "configfile: \"config/config.yaml\"\n",
    "```\n",
    "\n",
    "Snakemake will autmatically parse this out as a dictionary with the variable `config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30ab95",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Update the workflow \n",
    "\n",
    "First updated the scneario names in the target rule\n",
    "\n",
    "```python \n",
    "configfile: \"config/config.yaml\"\n",
    "SCENARIOS = [x for x in config[\"scenarios\"]]\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"results/{scenario}/AnnualCapacity.png\", scenario=SCENARIOS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560fb9a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, update the `params` in the rules that call the python scripts. Note that you can not directly evaluate expressions in `input`, `output`, `params` sections. Therefore, we use input functions. Exploring input functions is outside the scope of this workshop, but more information can be found [here](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#input-functions).\n",
    "\n",
    "```python\n",
    "rule capital_cost:\n",
    "    input:\n",
    "        \"resources/data/CapitalCost.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/data/CapitalCost.csv\"\n",
    "    params:\n",
    "        technology = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"capex\"][\"tech\"],\n",
    "        scaling_factor = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"capex\"][\"scale\"],\n",
    "    shell: \n",
    "        \"python workflow/scripts/capital_costs.py {input} {output} {params.technology} {params.scaling_factor}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45334de6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python \n",
    "rule emission_penalty:\n",
    "    input:\n",
    "        \"resources/data/EmissionsPenalty.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/data/EmissionsPenalty.csv\"\n",
    "    params:\n",
    "        start = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"emission_penalty\"][\"start\"],\n",
    "        end = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"emission_penalty\"][\"start\"],\n",
    "    shell: \n",
    "        \"python workflow/scripts/emission_penalty.py {input} {output} {params.start} {params.end}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf3499",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python \n",
    "rule demand:\n",
    "    input:\n",
    "        \"resources/data/SpecifiedAnnualDemand.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/data/SpecifiedAnnualDemand.csv\"\n",
    "    params:\n",
    "        scaling_factor = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"demand\"][\"scale\"],\n",
    "    shell: \n",
    "        \"python workflow/scripts/demand.py {input} {output} {params.scaling_factor}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba034a32",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Execute the Workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc72fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![dag-2](workshop/images/snakemake-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df345d2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "snakemake -c\n",
    "```\n",
    "<br>\n",
    "```bash\n",
    "Building DAG of jobs...\n",
    "Using shell: /usr/bin/bash\n",
    "Provided cores: 4\n",
    "Rules claiming more threads will be scaled down.\n",
    "Job stats:\n",
    "job                 count    min threads    max threads\n",
    "----------------  -------  -------------  -------------\n",
    "all                     1              1              1\n",
    "capital_cost            3              1              1\n",
    "copy_csv_files          3              1              1\n",
    "demand                  3              1              1\n",
    "emission_penalty        3              1              1\n",
    "otoole                  3              1              1\n",
    "plot                    3              1              1\n",
    "solve                   3              1              1\n",
    "variable_cost           3              1              1\n",
    "total                  25              1              1\n",
    "\n",
    "Select jobs to execute...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe9091",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Issues\n",
    "\n",
    "We have now created a generalized, flexible, and reproduciable workflow! <br>\n",
    "\n",
    "However, what happens if we want to itterate over LOTS of different parameter values? We could implement logic at the start of the script to generate many different permutations of values. OR we can introduce more wildcards! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635cee7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More Wildcards! \n",
    "\n",
    "The solution to this workflow can be found in `workshop/solutions/snakemake-4` and `workshop/solutions/config-2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060adf0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## File path structure\n",
    "\n",
    "Lets generalize all our parameter values and use them to build our scenario names. Our sceanrios will follow the structure. Note, that we no longer have `scenario` as a wildcard\n",
    "\n",
    "```bash\n",
    "d{scale_factor}/capex_{tech}{scale}/ep{start}/ep{end}/\n",
    "```\n",
    "\n",
    "So for example, results for the following scenario will be in the folder:\n",
    "- Scale demand by 2\n",
    "- Scale solar capaital costs by 0.5\n",
    "- Starting emission penalty of 0\n",
    "- Ending emission penalty of 100\n",
    "\n",
    "<br>\n",
    "```bash\n",
    "results/d2/capex_SPV0.5/ep0/ep100/AnnualCapacity.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d66145",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Configuration File\n",
    "\n",
    "We will also need to update our configuration file. We remove sceanrio names and replace them with ranges for our parameters to iterate over. \n",
    "\n",
    "```yaml\n",
    "scenarios:\n",
    "  capex:\n",
    "    techs: [\"SPV\", \"HYD\", \"GAS\"]\n",
    "    scale: [0.25, 4]\n",
    "  emission_penalty:\n",
    "    start: [0, 25]\n",
    "    end: [25, 100]\n",
    "  demand:\n",
    "    start: 0.5\n",
    "    end: 4.5\n",
    "    step: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b49555",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Target rule update\n",
    "\n",
    "```python\n",
    "configfile: \"config/config.yaml\"\n",
    "    \n",
    "TECHS = config[\"scenarios\"][\"capex\"][\"techs\"]\n",
    "CAPEX_SCALES = config[\"scenarios\"][\"capex\"][\"scale\"]\n",
    "EP_STARTS = config[\"scenarios\"][\"emission_penalty\"][\"start\"]\n",
    "EP_ENDS = config[\"scenarios\"][\"emission_penalty\"][\"end\"]\n",
    "D_SCALES = list(np.arange(\n",
    "    config[\"scenarios\"][\"demand\"][\"start\"], \n",
    "    config[\"scenarios\"][\"demand\"][\"end\"], \n",
    "    config[\"scenarios\"][\"demand\"][\"step\"])\n",
    ")\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"results/d{d_scale}/capex_{tech}{capex_scale}/ep{ep_start}/ep{ep_end}/AnnualCapacity.png\", \n",
    "            d_scale=D_SCALES,\n",
    "            tech=TECHS,\n",
    "            capex_scale=CAPEX_SCALES,\n",
    "            ep_start=EP_STARTS,\n",
    "            ep_end=EP_ENDS,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cd44e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Rule updates\n",
    "\n",
    "Repeat this structure for all rules!\n",
    "\n",
    "```python\n",
    "rule capital_cost:\n",
    "    input:\n",
    "        \"resources/data/CapitalCost.csv\"\n",
    "    output:\n",
    "        \"results/d{d_scale}/capex_{tech}{capex_scale}/ep{ep_start}/ep{ep_end}/data/CapitalCost.csv\"\n",
    "    shell: \n",
    "        \"python workflow/scripts/capital_costs.py {input} {output} {wildcards.tech} {wildcards.capex_scale}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a052a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Run the workflow \n",
    "\n",
    "```bash\n",
    "snakemake -c\n",
    "```\n",
    "\n",
    "```bash\n",
    "Job stats:\n",
    "job                 count    min threads    max threads\n",
    "----------------  -------  -------------  -------------\n",
    "all                     1              1              1\n",
    "capital_cost           72              1              1\n",
    "copy_csv_files         72              1              1\n",
    "demand                 72              1              1\n",
    "emission_penalty       72              1              1\n",
    "otoole                 72              1              1\n",
    "plot                   72              1              1\n",
    "solve                  72              1              1\n",
    "variable_cost          72              1              1\n",
    "total                 577              1              1\n",
    "\n",
    "Select jobs to execute...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330952b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In rule print outs, you can also see what wildcards are being subbed into the rule. \n",
    "\n",
    "```bash\n",
    "[Fri Mar 31 17:46:20 2023]\n",
    "rule capital_cost:\n",
    "    input: resources/data/CapitalCost.csv\n",
    "    output: results/d2.5/capex_SPV2/ep50/ep100/data/CapitalCost.csv\n",
    "    jobid: 1526\n",
    "    reason: Missing output files: results/d2.5/capex_SPV2/ep50/ep100/data/CapitalCost.csv\n",
    "    wildcards: d_scale=2.5, tech=SPV, capex_scale=2, ep_start=50, ep_end=100\n",
    "    resources: tmpdir=/tmp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931d514",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Issues \n",
    "\n",
    "While this works, do we really need to run all of the python scripts for each new scenario? No! The scenarios are sharing data and just creating different permutations! Lets expand our workflow to make it more efficient! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e32ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimize the workflow\n",
    "\n",
    "The solution to this workflow can be found in `workshop/solutions/snakemake-5` and `workshop/solutions/config-2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27c445",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Update the rules \n",
    "\n",
    "### Add a temporary directory\n",
    "\n",
    "Note that we could also use Snakemake's built in `temp` command (see [here](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#protected-and-temporary-files) for more info)\n",
    "\n",
    "```python\n",
    "rule create_temp_dir:\n",
    "    output:\n",
    "        directory(\"results/temp\")\n",
    "    shell:\n",
    "        \"mkdir output\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30dca3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Update Rules\n",
    "\n",
    "```python\n",
    "rule demand:\n",
    "    input:\n",
    "        \"resources/data/SpecifiedAnnualDemand.csv\"\n",
    "    output:\n",
    "        \"results/temp/demand_{d_scale}.csv\"\n",
    "    shell: \n",
    "        \"python workflow/scripts/demand.py {input} {output} {wildcards.d_scale}\"\n",
    "```\n",
    "<br>\n",
    "```python\n",
    "rule copy_demand_data:\n",
    "    input:\n",
    "        \"results/temp/demand_{d_scale}.csv\"\n",
    "    output:\n",
    "        \"results/d{d_scale}/capex_{tech}{capex_scale}/ep{ep_start}/ep{ep_end}/data/SpecifiedAnnualDemand.csv\"\n",
    "    shell:\n",
    "        \"cp {input} {output}\"\n",
    "```\n",
    "\n",
    "Same process for Emissions Penalty, Varaible Costs, and Captital Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd564bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Performance improvements\n",
    "\n",
    "By slightly changing the order, we reduced the number of times python scripts had to run from 72 * 4 = 288 to just 17. Thats a reduction of 94%!\n",
    "\n",
    "\n",
    "```bash\n",
    "Building DAG of jobs...\n",
    "Using shell: /usr/bin/bash\n",
    "Provided cores: 4\n",
    "Rules claiming more threads will be scaled down.\n",
    "Job stats:\n",
    "job                   count    min threads    max threads\n",
    "------------------  -------  -------------  -------------\n",
    "all                       1              1              1\n",
    "capital_cost              6              1              1\n",
    "copy_capex_data          72              1              1\n",
    "copy_csv_files           72              1              1\n",
    "copy_demand_data         72              1              1\n",
    "copy_emission_data       72              1              1\n",
    "copy_var_cost_data       72              1              1\n",
    "demand                    4              1              1\n",
    "emission_penalty          3              1              1\n",
    "otoole                   72              1              1\n",
    "plot                     72              1              1\n",
    "solve                    72              1              1\n",
    "variable_cost             4              1              1\n",
    "total                   594              1              1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f35dc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86f655",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Benifits \n",
    "Snakemake gives you the ability to eaisly creaty flexible, scalable, and reproduciable workflow without leaving python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ed3d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other Functionality \n",
    "- Snakemake scales easy to cloud and HPC infrastructure\n",
    "- Lots of other functionality (input functions, docker, flags, rule order, temporary files, modularity, Jupyter integration, wrapers, remote files, logging, wildcard constraints, ect...)\n",
    "- Create scenarios from csv files rather than yaml files \n",
    "- Powerful for uncertanity analysis, sensitivity analysis, and scenario analysis \n",
    "- Very easy way to make your work accessible and reproduciable (for you and others!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd03fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Helpful Rules \n",
    "\n",
    "```bash \n",
    "rule clean:\n",
    "    shell:\n",
    "        \"rm -rf results/*\"\n",
    "```\n",
    "<br>\n",
    "```bash\n",
    "rule make_dag:\n",
    "    shell:\n",
    "        \"snakemake --dag all | dot -Tpdf > dag.pdf\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca39f4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discussion!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
