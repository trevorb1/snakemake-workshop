{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10dba33d-29e8-4161-85a4-61c26f35160f",
   "metadata": {},
   "source": [
    "# Snakemake Workshop \n",
    "\n",
    "## Overview\n",
    "This presentation is intended to give an overview of the workflow management tool, [Snakemake](https://snakemake.readthedocs.io/en/stable/). \n",
    "\n",
    "## Audience \n",
    "Examples in this presentation are targeted at energy system modellers who are comfortable working with Python. This workshop will use the [OSeMOSYS](http://www.osemosys.org/) Energy Modelling tool and [otoole](https://otoole.readthedocs.io/en/latest/) Python package quite heavily. With that said, there is no background knowledge needed in energy system modelling to follow this example. \n",
    "\n",
    "## Motivation \n",
    "\"The Snakemake workflow management system is a tool to create reproducible and scalable data analyses.\" [ref](https://snakemake.readthedocs.io/en/stable/). Especially in the field of open-source energy modelling, ensuring data is reproducibale is often a requirment for publication. Introducting a tools to manage data pipelines not only simplifies your modelling workflow, but allows others to eaisly reporduce your results. \n",
    "\n",
    "## Similar tools\n",
    "Snakemake is a Python implementation of a workflow management tool started from the field of bioinfomatics. Many other competitors exist, such as [nextflow](https://www.nextflow.io/), [airflow](https://github.com/apache/airflow), [galaxy](https://usegalaxy.org/), or [mage](https://www.mage.ai/), often with specific use cases or audiances. \n",
    "\n",
    "## References\n",
    "Throughout this workshop, I will reference information from the following sources:\n",
    "- [Snakemake homepage](https://snakemake.github.io/)\n",
    "- [Snakemake paper](https://f1000research.com/articles/10-33)\n",
    "- [Snakemake docs](https://snakemake.readthedocs.io/en/stable/index.html)\n",
    "- [Reproduciable Data Analytic Workflows](https://lachlandeer.github.io/snakemake-econ-r-tutorial/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f43479af",
   "metadata": {},
   "source": [
    "# Background information \n",
    "\n",
    "## Common Terms \n",
    "- **Workflow**: A full data processing pipeline (a series of linked actions)\n",
    "- **Rule**: Rules decompose the workflow into small steps (for examples, running a single script) \n",
    "- **Directed Acyclic Graph (DAC)**: Defines how rules link together \n",
    "\n",
    "## Rule Structure \n",
    "In general a rule is structured following the template: \n",
    "\n",
    "```python \n",
    "rule name: \n",
    "    input: \n",
    "        \"input/file/path\"\n",
    "    output:\n",
    "        \"output/file/path\"\n",
    "    shell:\n",
    "        \"shell command\"\n",
    "```\n",
    "\n",
    "Instead of `shell`, options also include to directly run python code through `run`\n",
    "```python \n",
    "rule name: \n",
    "    input: \n",
    "        \"input/file/path\"\n",
    "    output:\n",
    "        \"output/file/path\"\n",
    "    run:\n",
    "        \"print('hello world')\"\n",
    "```\n",
    "\n",
    "Or to directly run scripts, if the correct directory structure is followed\n",
    "```python \n",
    "rule name: \n",
    "    input: \n",
    "        \"input/file/path\"\n",
    "    output:\n",
    "        \"output/file/path\"\n",
    "    script:\n",
    "        \"do_something.py\"\n",
    "```\n",
    "\n",
    "### Other Notes\n",
    "- Input and output variables are useable in the shell/run/script functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "123cf9fa-029b-4c9f-8d4d-42cc8553be0b",
   "metadata": {},
   "source": [
    "# Workflow Overview \n",
    "\n",
    "Suppose our workflow consists of four steps: \n",
    "1. Run a series of Python scripts to generate annual demand data, capital cost data, and variable fuel cost data for different scenarios.\n",
    "2. Convert all CSV data into a GNU MathProg datafile using otoole\n",
    "3. Solve the model through GLPK\n",
    "4. Visualize the total annual installed capacity\n",
    "\n",
    "Within this workflow, the generation of all other parameter data is held constant. Each scenario you intend to run will depend on user defined parameters that affect your parameters described in step one (ie. increased electrifiction scenario, cheap solar scenario, ect...). A graphical overview of the workflow is given below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4839b-f12a-4b81-8cde-00b91e4e96aa",
   "metadata": {},
   "source": [
    "# Model Overview \n",
    "In this example, we have a sinple three technology OSeMOSYS model. It consists of a solar powerplant, a natural gas powerplant, and a hydro powerplant, which all generate electricity to meet an end-use demand. The model is given below. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e97e867-cfc6-47f5-89d5-65a54471b41c",
   "metadata": {},
   "source": [
    "# Classical Workflow \n",
    "In many cases we will run each of these scenarios one-by-one, manually chnaging data in the script. We will first run through the steps needed to carry out the workflow. \n",
    "\n",
    "## 1. Create a Scenario Folder \n",
    "```bash \n",
    "mkdir scenarios \n",
    "```\n",
    "\n",
    "## 2. Copy of the reference data \n",
    "```bash \n",
    "cp resources/data scenarios/data\n",
    "```\n",
    "\n",
    "## 3. Run the capital costs script\n",
    "In this case we will implement a 1.5 scaling factor on solar panels\n",
    "\n",
    "```bash \n",
    "python capital_costs.py scenarios/data/CapitalCosts.csv scenarios/data/CapitalCosts.csv SPV 1.5\n",
    "```\n",
    "\n",
    "## 4. Run the emission penalty script\n",
    "In this case we will create a linear increase from 0 to 100 $/T\n",
    "\n",
    "```bash \n",
    "python emission_penalty.py scenarios/data/EmissionsPenalty.csv scenarios/data/EmissionsPenalty.csv 0 100\n",
    "```\n",
    "## 5. Run the demand script\n",
    "In this case we will scale the demand by 2\n",
    "\n",
    "```bash \n",
    "python demand.py scenarios/data/SpecifiedAnnualDemand.csv scenarios/data/SpecifiedAnnualDemand.csv 2\n",
    "```\n",
    "\n",
    "## 6. Run the variable costs script\n",
    "Note that this script must be run after the demand script \n",
    "\n",
    "```bash \n",
    "python variable_costs.py scenarios/data/SpecifiedAnnualDemand.csv scenarios/data/VariableCosts.csv scenarios/data/VariableCosts.csv\n",
    "```\n",
    "\n",
    "## 7. Use otoole to create a datafile \n",
    "\n",
    "```bash \n",
    "otoole convert csv datafile scenarios/data scenarios/data.txt resources/config.yaml\n",
    "```\n",
    "\n",
    "## 8. Create a folder to hold result data\n",
    "\n",
    "```bash \n",
    "mkdir scenarios/results\n",
    "```\n",
    "\n",
    "## 9. Run GLPK to build the model \n",
    "OSeMOSYS requires the current working directory to have a `results`\n",
    "folder when solving through GLPK, so we first change directories to \n",
    "the scenario folder \n",
    "\n",
    "```bash \n",
    "cd scenarios\n",
    "glpsol -m ../resources/osemosys.txt -d scenarios/\n",
    "cd ..\n",
    "```\n",
    "\n",
    "## 9. Run capacity plotting script \n",
    "\n",
    "```bash \n",
    "python plot_capacity.py scenarios/results/AnnualCapacity.csv scenarios/AnnualCapacity.png\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "301dd103-8acc-487f-a634-6711e5896c81",
   "metadata": {},
   "source": [
    "## Issues\n",
    "1. Running many scenarios becomes a nightmear \n",
    "2. Easy to make data mistakes (forgetting to run a script)\n",
    "3. Hard to replicate results "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3e893bc-cc28-427f-9d39-f9b3c2d7927c",
   "metadata": {},
   "source": [
    "# Snakemake Workflow\n",
    "Lets automate this process through the use of a snakemake workflow! Snakemake looks for a a file (called a `snakefile`) located either in the root directory or the `workflow` directory. This file has already been created at `workflow/snakefile`\n",
    "\n",
    "## 1. Create a \"target\" rule\n",
    "All snakemake files should have a master rule (called the target) which represents the end goal of the workflow. In this case it is a chart of the capacity. \n",
    "\n",
    "```python\n",
    "rule all:\n",
    "    input:\n",
    "        \"results/scenario/AnnualCapacity.png\"\n",
    "```\n",
    "\n",
    "## 2. Create the rules to execute the scripts \n",
    "Rememmber, that the variable cost script must be run after the demand script! \n",
    "\n",
    "```python\n",
    "rule capital_cost:\n",
    "    input:\n",
    "        \"resources/data/AnnualDemand.csv\"\n",
    "    output:\n",
    "        \"results/scenario/data/AnnualDemand.csv\"\n",
    "    params:\n",
    "        technology = \"SPV\",\n",
    "        scaling_factor = 1.5\n",
    "    shell: \n",
    "        \"python workflow/scripts/capital_costs.py {input} {output} {params.technology} {params.scaling_factor}\"\n",
    "```\n",
    "\n",
    "```python\n",
    "rule emission_penalty:\n",
    "    input:\n",
    "        \"resources/data/EmissionsPenalty.csv\"\n",
    "    output:\n",
    "        \"results/scenario/data/EmissionsPenalty.csv\"\n",
    "    params:\n",
    "        start = 0,\n",
    "        end = 100\n",
    "    shell: \n",
    "        \"python workflow/scripts/emission_penalty.py {input[0]} {output[0]} {params.start} {params.end}\"\n",
    "```\n",
    "\n",
    "```python\n",
    "rule demand:\n",
    "    input:\n",
    "        \"resources/data/AnnualDemand.csv\"\n",
    "    output:\n",
    "        \"results/scenario/data/AnnualDemand.csv\"\n",
    "    params:\n",
    "        scaling_factor = 2,\n",
    "    shell: \n",
    "        \"python workflow/scripts/demand.py {input} {output} {params.scaling_factor}\"\n",
    "```\n",
    "\n",
    "```python\n",
    "rule variable_cost:\n",
    "    input:\n",
    "        var_cost = \"resources/data/VariableCosts.csv\",\n",
    "        demand = \"results/scenario/data/AnnualDemand.csv\"\n",
    "    output:\n",
    "        \"results/scenario/data/VariableCosts.csv\"\n",
    "    shell: \n",
    "        \"python workflow/scripts/variable_costs.py {input.demand} {input.var_cost} {output}\"\n",
    "```\n",
    "\n",
    "## 3. Create the remaining files\n",
    "In general, Snakemake rule outputs must be unique, meaning that the same file shouldn't be created through multiple rules. Therefore, we need to be careful not to create a rule that also outputs the files `AnnualDemand.csv`, `CapitalCosts.csv`, `EmissionsPenalty.csv`, and `VariableCosts.csv`.\n",
    "\n",
    "We first create constants of the files we need: \n",
    "```python\n",
    "OSEMOSYS_CSVS = os.listdir(\"resources/data\")\n",
    "CSVS_TO_CREATE = [\n",
    "    \"AnnualDemand.csv\",\n",
    "    \"EmissionsPenalty.csv\",\n",
    "    \"VariableCosts.csv\",\n",
    "    \"CapitalCosts.csv\"\n",
    "]\n",
    "CSVS_TO_COPY = [f for f in OSEMOSYS_CSVS if f not in CSVS_TO_CREATE]\n",
    "```\n",
    "\n",
    "And create a rule to only copy over the files in `CSVS_TO_COPY`:\n",
    "```python \n",
    "rule copy_csv_files:\n",
    "    input:\n",
    "        expand(\"resources/data/{csv}\", csv=OSEMOSYS_CSVS)\n",
    "    output:\n",
    "        expand(\"results/scenario/data/{csv}\", csv=CSVS_TO_COPY)\n",
    "    params:\n",
    "        folder = directory(\"results/scenario/data\")\n",
    "    run:\n",
    "        for path in input:\n",
    "            _, f = os.path.split(path) # f will be in form of \"file.csv\"\n",
    "            if f in CSVS_TO_CREATE:\n",
    "                continue\n",
    "            shutil.copy(path, os.path.join(params.folder, f))\n",
    "```\n",
    "\n",
    "## 4. Create the datafile rule \n",
    "```python \n",
    "rule otoole:\n",
    "    input:\n",
    "        expand(\"results/scenario/data/{csv}\", csv=OSEMOSYS_CSVS)\n",
    "    output:\n",
    "        \"results/scenario/data.txt\"\n",
    "    params:\n",
    "        csv_dir = \"results/scenario/data\",\n",
    "        config=\"resources/config.yaml\"\n",
    "    shell:\n",
    "        \"otoole convert csv datafile {params.csv_dir} {output} {params.config}\"\n",
    "```\n",
    "\n",
    "## 5. Solve the model \n",
    "```python \n",
    "rule solve:\n",
    "    input: \n",
    "        \"results/scenario/data.txt\"\n",
    "    output:\n",
    "        \"results/scenario/results/TotalCapacityAnnual.csv\"\n",
    "    params:\n",
    "        model=\"resources/osemosys.txt\"\n",
    "    shell:\n",
    "        # since OSeMOSYS using GLPK requires a results directory from the \n",
    "        # place of running, we change the working directory to the location of the scenario\n",
    "        \"\"\"\n",
    "        FILE=\"resources/data.txt\" &&\n",
    "        f=\"$(basename -- $FILE)\" &&\n",
    "        cd results/scenario &&\n",
    "        glpsol -m ../../{params.model} -d $f\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "## 6. Plot Results\n",
    "```python\n",
    "rule plot:\n",
    "    input:\n",
    "        \"results/scenario/results/TotalCapacityAnnual.csv\"\n",
    "    output:\n",
    "        \"results/scenario/AnnualCapacity.png\"\n",
    "    shell:\n",
    "        \"python workflow/scripts/plot_capacity.py {input} {output} 'Scenario'\"\n",
    "```\n",
    "\n",
    "## 7. Create missing directories \n",
    "```python\n",
    "rule create_scenario_dir:\n",
    "    output:\n",
    "        directory(\"results/scenario/data\")\n",
    "    shell:\n",
    "        \"mkdir output\"\n",
    "```\n",
    "\n",
    "```python\n",
    "rule results_dir:\n",
    "    input:\n",
    "        \"results/scenario\"\n",
    "    output:\n",
    "        directory(\"results/scenario/results\")\n",
    "    shell:\n",
    "        \"mkdir {output}\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59053ea9",
   "metadata": {},
   "source": [
    "# Visualize the workflow \n",
    "Before executing lets visualize the workflow\n",
    "```bash\n",
    "snakemake --dag all | dot -Tpdf > dag.pdf\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ad8d26d",
   "metadata": {},
   "source": [
    "# Execute the workflow \n",
    "The `-c` flag sets the number of cores to use in the workflow. If no value is given (as shown below), snakemake will use all availbale cores. If you only wanted 2 cores to be used, you would set the flag as `-c2`. \n",
    "```bash \n",
    "snakemake -c\n",
    "```\n",
    "\n",
    "```bash\n",
    "Building DAG of jobs...\n",
    "Using shell: /usr/bin/bash\n",
    "Provided cores: 4\n",
    "Rules claiming more threads will be scaled down.\n",
    "Job stats:\n",
    "job                 count    min threads    max threads\n",
    "----------------  -------  -------------  -------------\n",
    "all                     1              1              1\n",
    "capital_cost            1              1              1\n",
    "copy_csv_files          1              1              1\n",
    "demand                  1              1              1\n",
    "emission_penalty        1              1              1\n",
    "otoole                  1              1              1\n",
    "plot                    1              1              1\n",
    "solve                   1              1              1\n",
    "variable_cost           1              1              1\n",
    "total                   9              1              1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues \n",
    "We are still hardcoding in variable values. This is a reproduciable workflow, but not a flexible workflow. Lets fix that with wildcards and a configuration file! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1965d684",
   "metadata": {},
   "source": [
    "# Wildcards\n",
    "Usually, it is useful to generalize a rule to be applicable to a number of e.g. datasets. For this purpose, wildcards can be used. Automatically resolved multiple named wildcards are a key feature and strength of Snakemake in comparison to other systems. ([Source](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#wildcards))\n",
    "\n",
    "Note that the target rule can not include wildcards. We need to explicitly tell Snakemake what files we want to create in the target rule. \n",
    "\n",
    "## 1. Change our Scenario name to a wildcard\n",
    "**DO NOT CHANGE THE TARGET RULE TO INCLUDE WILDCARDS**\n",
    "```python\n",
    "rule capital_cost:\n",
    "    input:\n",
    "        \"resources/data/CapitalCost.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/data/CapitalCost.csv\"\n",
    "    params:\n",
    "        technology = \"SPV\",\n",
    "        scaling_factor = 1.5\n",
    "    shell: \n",
    "        \"python workflow/scripts/capital_costs.py {input} {output} {params.technology} {params.scaling_factor}\"\n",
    "```\n",
    "\n",
    "```python\n",
    "rule otoole:\n",
    "    input:\n",
    "        expand(\"results/{{scenario}}/data/{csv}\", csv=OSEMOSYS_CSVS)\n",
    "    output:\n",
    "        \"results/{scenario}/data.txt\"\n",
    "    params:\n",
    "        csv_dir = \"results/{scenario}/data\",\n",
    "        config=\"resources/config.yaml\"\n",
    "    shell:\n",
    "        \"otoole convert csv datafile {params.csv_dir} {output} {params.config}\"\n",
    "```\n",
    "\n",
    "```python\n",
    "rule plot:\n",
    "    input:\n",
    "        \"results/{scenario}/results/TotalCapacityAnnual.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/AnnualCapacity.png\"\n",
    "    shell:\n",
    "        \"python workflow/scripts/plot_capacity.py {input} {output} {wildcards.scenario}\"\n",
    "```\n",
    "\n",
    "## 2. Hard code in scenarios\n",
    "```python\n",
    "SCENARIOS = [\"Kamaria\", \"Teddy\", \"Pierre\", \"Narges\", \"Yalda\", \"Trevor\", \"Elias\"]\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"results/{scenario}/AnnualCapacity.png\", scenario=SCENARIOS)\n",
    "```\n",
    "\n",
    "## 3. Visualize the workflow \n",
    "```bash\n",
    "snakemake --dag all | dot -Tpdf > dag.pdf\n",
    "```\n",
    "\n",
    "## 4. Run the workflow \n",
    "```bash \n",
    "snakemake -c\n",
    "```\n",
    "\n",
    "```bash\n",
    "Building DAG of jobs...\n",
    "Using shell: /usr/bin/bash\n",
    "Provided cores: 4\n",
    "Rules claiming more threads will be scaled down.\n",
    "Job stats:\n",
    "job                 count    min threads    max threads\n",
    "----------------  -------  -------------  -------------\n",
    "all                     1              1              1\n",
    "capital_cost            4              1              1\n",
    "copy_csv_files          4              1              1\n",
    "demand                  4              1              1\n",
    "emission_penalty        4              1              1\n",
    "otoole                  4              1              1\n",
    "plot                    4              1              1\n",
    "solve                   4              1              1\n",
    "variable_cost           4              1              1\n",
    "total                  33              1              1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efe307a9",
   "metadata": {},
   "source": [
    "# Configuration File \n",
    "We have now introduced flexibility and modularity into our workflow, but it still requires us to hard code in values for the scenario name, scaling factor, ect. Lets move all this information to a configuration file! \n",
    "\n",
    "## Configuartion Setup \n",
    "First, set up a `config/config.yaml` file\n",
    "\n",
    "```yaml\n",
    "scenarios:\n",
    "  scenario_one:\n",
    "    capex:\n",
    "      tech: \"SPV\"\n",
    "      scale: 1.5\n",
    "    emission_penalty:\n",
    "      start: 0\n",
    "      end: 100\n",
    "    demand:\n",
    "      scale: 2\n",
    "  scenario_two:\n",
    "    ...\n",
    "```\n",
    "\n",
    "## Import into workflow \n",
    "Snakemake will autmatically parse this out as a dictionary with the variable `config`\n",
    "```python\n",
    "configfile: \"config/config.yaml\"\n",
    "```\n",
    "\n",
    "## Update workflow \n",
    "You can not directly evaluate expressions in `input`, `output`, `params` sections. Therefore, we\n",
    "use input functions.\n",
    "\n",
    "```python\n",
    "rule capital_cost:\n",
    "    input:\n",
    "        \"resources/data/CapitalCost.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/data/CapitalCost.csv\"\n",
    "    params:\n",
    "        technology = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"capex\"][\"tech\"],\n",
    "        scaling_factor = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"capex\"][\"scale\"],\n",
    "    shell: \n",
    "        \"python workflow/scripts/capital_costs.py {input} {output} {params.technology} {params.scaling_factor}\"\n",
    "```\n",
    "\n",
    "```python \n",
    "rule emission_penalty:\n",
    "    input:\n",
    "        \"resources/data/EmissionsPenalty.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/data/EmissionsPenalty.csv\"\n",
    "    params:\n",
    "        start = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"emission_penalty\"][\"start\"],\n",
    "        end = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"emission_penalty\"][\"start\"],\n",
    "    shell: \n",
    "        \"python workflow/scripts/emission_penalty.py {input} {output} {params.start} {params.end}\"\n",
    "```\n",
    "\n",
    "```python \n",
    "rule demand:\n",
    "    input:\n",
    "        \"resources/data/SpecifiedAnnualDemand.csv\"\n",
    "    output:\n",
    "        \"results/{scenario}/data/SpecifiedAnnualDemand.csv\"\n",
    "    params:\n",
    "        scaling_factor = lambda wildcards: config[\"scenarios\"][wildcards.scenario][\"demand\"][\"scale\"],\n",
    "    shell: \n",
    "        \"python workflow/scripts/demand.py {input} {output} {params.scaling_factor}\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6931d514",
   "metadata": {},
   "source": [
    "# More Wildcards! \n",
    "What happens if we want to itterate over LOTS of different parameter values? We could \n",
    "implement logic at the start of the script to generate many different permutations of \n",
    "values. OR we can just introduce more wildcards! \n",
    "\n",
    "Lets generalize all our parameter values and use them to build our scenario names. Our sceanrios\n",
    "will follow the structure: \n",
    "`d{scale_factor}/capex_{tech}{scale}/ep{start}/ep{end}/`\n",
    "\n",
    "## Configuration File\n",
    "```yaml\n",
    "scenarios:\n",
    "  capex:\n",
    "    techs: [\"SPV\", \"HYD\", \"GAS\"]\n",
    "    scale: [0.25, 4]\n",
    "  emission_penalty:\n",
    "    start: [0, 25]\n",
    "    end: [25, 100]\n",
    "  demand:\n",
    "    start: 0.5\n",
    "    end: 4.5\n",
    "    step: 1\n",
    "```\n",
    "\n",
    "## Workflow Update\n",
    "\n",
    "\n",
    "## Visualize Workflow \n",
    "\n",
    "\n",
    "## Run Workflow \n",
    "```bash\n",
    "snakemake -c\n",
    "```\n",
    "\n",
    "```yaml\n",
    "Building DAG of jobs...\n",
    "Using shell: /usr/bin/bash\n",
    "Provided cores: 4\n",
    "Rules claiming more threads will be scaled down.\n",
    "Job stats:\n",
    "job                 count    min threads    max threads\n",
    "----------------  -------  -------------  -------------\n",
    "all                     1              1              1\n",
    "capital_cost          360              1              1\n",
    "copy_csv_files        360              1              1\n",
    "demand                360              1              1\n",
    "emission_penalty      360              1              1\n",
    "otoole                360              1              1\n",
    "plot                  360              1              1\n",
    "solve                 360              1              1\n",
    "variable_cost         360              1              1\n",
    "total                2881              1              1\n",
    "\n",
    "Select jobs to execute...\n",
    "\n",
    "[Fri Mar 31 17:46:20 2023]\n",
    "rule capital_cost:\n",
    "    input: resources/data/CapitalCost.csv\n",
    "    output: results/d2.5/capex_SPV2/ep50/ep100/data/CapitalCost.csv\n",
    "    jobid: 1526\n",
    "    reason: Missing output files: results/d2.5/capex_SPV2/ep50/ep100/data/CapitalCost.csv\n",
    "    wildcards: d_scale=2.5, tech=SPV, capex_scale=2, ep_start=50, ep_end=100\n",
    "    resources: tmpdir=/tmp\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82f35dc7",
   "metadata": {},
   "source": [
    "# Other Notes\n",
    "- Snakemake scales easy to cloud infrastructure\n",
    "- Lots of other functionality (input functions, docker, flags, rule order, temporary files, modularity, Jupyter integration, wrapers, remote files, ect...)\n",
    "- Create scenarios from csv files rather than yaml files \n",
    "- Powerful for uncertanity analysis, sensitivity analysis, and scenario analysis functions \n",
    "- Only reruns necessary rules \n",
    "- Very easy way to make your work accessible "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
